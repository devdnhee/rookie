\chapter{Introduction}

Ever since computer science started to emerge as a proper field, researchers have been attracted to the problem of letting computers play the game of chess. This is mainly out of theoretical interest for \gls{ai}, as it is a human task. Chess is a complex game (it has been estimated that around $10^{120}$ unique board positions exist \cite{shann50}), which makes brute force algorithms completely unscalable. In the early 50's, notable scientists such as Turing and Shannon created a theoretical chess program on paper \cite{shann50,tur53}. Remarkable is they both already included the notions of an evaluation function, minimax and quiescent search. These concepts are still used today in state of the art chess programs. \\

In the second half of the 20th century, computer chess evolved further and further. Continuous minor adjustments were made to the heuristics of the evaluation function, computation became more and more parallelized and search methods were improved with more efficient pruning. This led to the victory of \textit{Deep Blue} against world champion at that time Gary Kasparov in 1997 \cite{dblue02}, an event that received a lot of attention from the media, mainly due to the symbolical significance of machine catching up with human intelligence \cite{wstd97}. \\

Since that event, computer chess was studied less and less and the \textit{Drosophila} \footnote{Drosophila species are extensively used as model organisms in genetics for testing new techniques. It is used as a metaphor in this context.} of \gls{ai} research became go, a game that is, despite its simplicity of rules, extremely complex. Search techniques classically used in chess engines are less usable for go engines mainly due to the size of the board and large amount of possible moves every ply. Recently in 2016, a \textit{Google} research team succeeded at beating a 9-dan \footnote{Highest rating achievable in \textit{go}.} professional player \cite{alphago16}. The researchers achieved this by using a combination of tree search algorithms and deep \gls{rl}. This success lead to a nomination by \textit{Science} magazine for \textit{Breaktrough of the Year} \cite{scboy16}. \\

In this master thesis, we look into how \acrlong{rl} and other \acrlong{ml} techniques could be used on chess as well. Through self play and deep neural networks for the evaluation function we investigate what machines can learn about the chess game without any prior knowledge except the rules. This lack of inserting human knowledge into the system makes the learning agent as unbiased as possible, which is necessary to truly reveal the power of \gls{rl}. In a similar manner by a huge amount of stochastic self play combined with a TD learning algorithm \footnote{A \gls{rl} algorithm, further discussed in chapter \ref{ch:background}}, \textit{TD-gammon} was able to discover new strategies humans never thought about before \cite{tdgammon95}.\\

The move decision of current chess computers is based on looking ahead as far as possible. They build a game tree and evaluate positions at the bottom of the tree with a static evaluation function. The move choice is then the one that would lead to the best evaluation at the bottom of the tree under the assumption that both players try to make the best moves. the search is heavily optimized as an attempt to reduce the influence of the exponentially increasing complexity when looking more moves ahead. The height of the search tree is the most crucial factor for the performance of an engine.\\

The evaluation function is usually a heuristic based where game technical knowledge is used, which has been gathered during the history of the game by expert chess players. These rules are not objective, as they were determined by humans. On top of that, chess computers have difficulties at playing intuitively.. They still show signs of weakness in the endgame, and it takes a lot of effort to design or improve them. Furthermore, porting these technique to other games requires redesigning most of the heuristics, and the resulting \glspl{ai} do not always achieve a similar high level of play for more strategical games.\\

All current machine learning approaches to learn the static evaluation function use the helping hand of human knowledge about the chess game in their learning system. This makes it harder to actually improve on the classical state of the art approaches with heuristics. The purpose of this thesis is to research how a machine could teach itself to play chess with the rules as only knowledge. Choosing for a completely unbiased system could make the chess engine learn strategies humans never thought about before, just like what happened with \textit{TD-Gammon}. \\

There have been recent and less recent efforts to incorporate machine learning into chess engines. Recently in 2015, an end-to-end machine learning chess engine incorporating self play and reinforcement learning has reached the FIDE International Master level, while Grandmaster level can be achieved with the use of millions of database games \cite{giraffe15,deepchess16}. \\

Deep \gls{rl} can help us out with our quest at perfecting chess play and strategy. In \gls{rl}, agents are released in an environment where they have to find out by exploration which actions generate the best cumulative rewards. To improve their play, the agents make use of the gained experience. In this case, the board is the environment and white and black are the agents. By only incorporating basic and objective game elements into the framework the agents can discover by themselves what is good and what not, without additional biased knowledge. By using self play in a Deep \gls{rl} framework a value function can be constructed with only information observed by the agents themselves. As this would all be learned by the agents experience, the final static evaluation could introduce additional depth information, making deeper searches less necessary. \\
%TODO:kort overzicht sota van huidige chess engines (hoe werken ze ongeveer)
%TODO:redenen/motivatie waarom rl hierbij kan helpen


%TODO eventueel iets meer sota ivm deep rl (vb atari)
Deep \gls{rl} has already proven its worth in games, \textit{Atari} and \textit{Go} being most noteworthy. The fact that sensory image data is being processed during playing \textit{Atari} games made it the first game where \glspl{cnn} were proven successful \cite{silv13}. These video games are an excellent toy to mess around and experiment with new deep \gls{rl} algorithms. \gls{ai} game play on \textit{Atari} games has evolved further and new architectures came along the way \cite{dddqn15}.\\

In this master's thesis we will set the first step in the quest to let machines master the complete chess game through self play only and gaining experience without biased supervision. More specifically, we introduce new algorithms that can be used along the way. Due to time constraints and complexity of the overall problem, it has not been possible to implement and examine all covered techniques, but we compare a new \gls{td} learning algorithm, TD-Stem($\lambda$), with a state of the art mechanism to improve chess play with \gls{rl}. We also limit ourselves to easy chess endgame problems, as they give the opportunity for objective quality assessments. The learned models could be used to learn more complex chess positions as well from the bottom up.\\
The main questions we try to answer in this dissertation are:
\begin{itemize}
\item How would one program a chess engine without bias with deep \gls{rl}?
\item What is the best TD learning algorithm for this cause?
\end{itemize}

This book is organized as follows. We start off in chapter \ref{ch:background} by exploring how current chess computers achieve their strength and further discuss the remaining weaknesses. In chapter \ref{ch:background} an overview is given of deep learning. The knowledge gained in these two chapters are combined in chapter \ref{ch:drlgames}, where existing and new ideas are combined to propose possible architectures and techniques for programming a chess computer with deep \gls{rl}. The setup of the carried out experiments are further discussed in chapter \ref{ch:experiments}. Subsequently, the results of these experiments are presented and discussed in chapter \ref{ch:res}. This thesis is finalized in a conclusion in chapter \ref{ch:conclusion}.

For the readers whom are less familiar with the game of chess, we provide a brief introduction in appendix \ref{ch:chess_app}.






