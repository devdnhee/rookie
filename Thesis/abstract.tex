\newpage
\vspace*{2cm}
\begingroup
    \fontsize{11pt}{13.2pt}\selectfont


\begin{center}
    \textbf{{\huge  Teaching Computers to Play Chess with Deep Reinforcement Learning}}\\[.5em]
   
    Dorian~\textsc{Van den Heede}\\[.5em]
   
    Master's dissertation submitted in order to obtain the academic degree of\\[.1em]\textsc{Master of Science in Computer Science Engineering}\\[.5em]
   
    Academic year 2016-2017\\[.5em]
   
    {Supervisors:} Prof. dr. ir. F. \textsc{wyffels} and Prof. dr. ir. J. \textsc{Dambre}\\[.1em]
    {Counsellor:} J. \textsc{Burms}\\[.5em]
 
    Faculty of Engineering and Architecture\\[.1em]
    Ghent University\\[.5em]
   
    Department of Electronics and Information Systems\\[.1em]
    Chair: Prof. dr. ir. R. \textsc{Van de Walle}
\end{center}
 
 
\setlength{\parskip}{1.5ex}
 
\subsection*{Abstract}
 
Chess computers have reached a superhuman level at playing chess by combining tree search with the incorporation of heuristics obtained with expert knowledge into the value function when evaluating board positions. Although these approaches add a big amount of tactical knowledge, they do not tackle the problem of effectively solving chess, a deterministic game. We try to set the first stone by solving relatively simple chess endgames from a human point of view with deep reinforcement learning (DRL). The proposed DRL algorithm consists of 2 main components: (i) self play (ii) translating the collected data in phase (i) to a supervised learning framework with deep learning. In light of this, we present a novel TD learning algorithm, TD-Stem($\lambda$), and compare it with the state of the art, TD-Leaf($\lambda$).
 
\setlength{\parskip}{0ex}
 
\subsection*{Keywords}
 
Chess, deep reinforcement learning, TD-learning